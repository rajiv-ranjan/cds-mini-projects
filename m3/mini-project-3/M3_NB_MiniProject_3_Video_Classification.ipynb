{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFBl3DsqB3AE"
   },
   "source": [
    "# Advanced Certification Program in Computational Data Science\n",
    "\n",
    "##  A program by IISc and TalentSprint\n",
    "\n",
    "### Mini Project Notebook: Video based Action Classification using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maritime-miami"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95F1ym6qB8VU"
   },
   "source": [
    "At the end of the experiment, you will be able to :\n",
    "\n",
    "* extract frames out of a video\n",
    "* build the CNN model to extract features from the video frames\n",
    "* train LSTM/GRU model to perform action classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8aczZmzvXTc"
   },
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0Ld7v8N6Z12"
   },
   "source": [
    "**Background:** The CNN LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction.\n",
    "\n",
    "CNN LSTMs were developed for visual time series prediction problems and the application of generating textual descriptions from sequences of images (e.g. videos). Specifically, the problems of:\n",
    "\n",
    "\n",
    "\n",
    "*   Activity Recognition: Generating a textual description of an activity demonstrated in a sequence of images\n",
    "*   Image Description: Generating a textual description of a single image.\n",
    "*   Video Description: Generating a textual description of a sequence of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwfDjPVOaTDV"
   },
   "source": [
    "**Applications:** Applications such as surveillance, video retrieval and\n",
    "human-computer interaction require methods for recognizing human actions in various scenarios. In the area of robotics, the tasks of\n",
    "autonomous navigation or social interaction could also take advantage of the knowledge extracted\n",
    "from live video recordings. Typical scenarios\n",
    "include scenes with cluttered, moving backgrounds, nonstationary camera, scale variations, individual variations in\n",
    "appearance and cloth of people, changes in light and view\n",
    "point and so forth. All of these conditions introduce challenging problems that can be addressed using deep learning (computer vision) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgx1PkHfCDyJ"
   },
   "source": [
    "## Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adc87eHGA_NY"
   },
   "source": [
    "**Dataset:** This dataset consists of labelled videos of 6 human actions (walking, jogging, running, boxing, hand waving and hand clapping) performed several times by 25 subjects in four different scenarios: outdoors s1, outdoors with scale variation s2, outdoors with different clothes s3 and indoors s4 as illustrated below.\n",
    "\n",
    "![img](https://cdn.iisc.talentsprint.com/CDS/Images/actions.gif)\n",
    "\n",
    "All sequences were taken over homogeneous backgrounds with a static camera with 25fps frame rate. The sequences were downsampled to the spatial resolution of 160x120 pixels and have a length of four seconds in average. In summary, there are 25x6x4=600 video files for each combination of 25 subjects, 6 actions and 4 scenarios. For this mini-project we have randomly selected 20% of the data as test set.\n",
    "\n",
    "Dataset source: https://www.csc.kth.se/cvap/actions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN7vlh6jB40i"
   },
   "source": [
    "**Methodology:**\n",
    "\n",
    "When performing image classification, we input an image to our CNN; Obtain the predictions from the CNN;\n",
    "Choose the label with the largest corresponding probability\n",
    "\n",
    "\n",
    "Since a video is just a series of image frames, in a video classification, we Loop over all frames in the video file;\n",
    "For each frame, pass the frame through the CNN; Classify each frame individually and independently of each other; Choose the label with the largest corresponding probability;\n",
    "Label the frame and write the output frame to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoLphNfQF_28"
   },
   "source": [
    "Refer this [Video Classification using Keras](https://medium.com/video-classification-using-keras-and-tensorflow/action-recognition-and-video-classification-using-keras-and-tensorflow-56badcbe5f77) for complete understanding and implementation example of video classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih-oasWmdZul"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-18cjyCTCHE-"
   },
   "source": [
    "Train a CNN-LSTM based deep neural net to recognize the action being performed in a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "operating-latter"
   },
   "source": [
    "## Grading = 10 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tKT4MlqAbub"
   },
   "source": [
    "### Install and re-start the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pgxCas9hAeUq"
   },
   "outputs": [],
   "source": [
    "# !pip3 install imageio==2.4.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "kX5ljpgMqkxh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Download Dataset\n",
    "# !wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/Actions.zip\n",
    "# !unzip -qq Actions.zip\n",
    "# print(\"Dataset downloaded successfully!!\")\n",
    "\n",
    "\n",
    "from utility import download_and_unzip\n",
    "\n",
    "download_and_unzip(\n",
    "    filename=\"Actions.zip\",\n",
    "    url=\"https://cdn.iisc.talentsprint.com/CDS/MiniProjects/Actions.zip\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abstract-stocks"
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2LgMamdMvHRv"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os, glob\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ_RWsv1vsFI"
   },
   "source": [
    "### Load the data and generate frames of video (2 points)\n",
    "\n",
    "Detecting an action is possible by analyzing a series of images (that we name “frames”) that are taken in time.\n",
    "\n",
    "Hint: Refer data preparation section in [keras_video_classification](https://keras.io/examples/vision/video_classification/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6Sqyn9UAsXy4"
   },
   "outputs": [],
   "source": [
    "# data_dir = \"/content/Actions/train/\"\n",
    "# test_data_dir = \"/content/Actions/test/\"\n",
    "# YOUR CODE HERE\n",
    "data_dir = \"Actions_temp/train/\"\n",
    "test_data_dir = \"Actions_temp/test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(video_path):\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        cv.imshow(\"frame\", gray)\n",
    "        if cv.waitKey(25) == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "# play_video(data_dir + \"boxing/person01_boxing_d1_uncomp.avi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_into_array_(file_path):\n",
    "    # Load the video from the file path\n",
    "    cap = cv.VideoCapture(file_path)\n",
    "\n",
    "    # Initialize an empty list to store the frames\n",
    "    frames = []\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read the frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If the frame was not read correctly, break the loop\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Resize the frame to 224x224\n",
    "        frame = cv.resize(frame, (224, 224))\n",
    "\n",
    "        # Normalize the pixel values\n",
    "        frame = frame / 255.0\n",
    "\n",
    "        # Append the frame to the frames list\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Release the VideoCapture object and close the window\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    # Convert the frames list to a numpy array\n",
    "    frames = np.array(frames)\n",
    "\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_into_dataframe(root_path):\n",
    "    # Initialize an empty list to store the file names and labels\n",
    "    data = []\n",
    "\n",
    "    # Loop through each subfolder in the folder\n",
    "    for subfolder in os.listdir(root_path):\n",
    "        subfolder_path = os.path.join(root_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Loop through each file in the subfolder\n",
    "            for file_name in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Append the file name and label (subfolder name) to the data list\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"folder\": subfolder_path,\n",
    "                            \"file_name\": file_name,\n",
    "                            \"label\": subfolder,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    # Create a dataframe from the data list\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "folder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "file_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d963b832-5248-4f0d-882d-69629d87bed1",
       "rows": [
        [
         "0",
         "Actions_temp/train/running",
         "person01_running_d3_uncomp.avi",
         "running"
        ],
        [
         "1",
         "Actions_temp/train/running",
         "person01_running_d1_uncomp.avi",
         "running"
        ],
        [
         "2",
         "Actions_temp/train/running",
         "person01_running_d2_uncomp.avi",
         "running"
        ],
        [
         "3",
         "Actions_temp/train/handwaving",
         "person01_handwaving_d2_uncomp.avi",
         "handwaving"
        ],
        [
         "4",
         "Actions_temp/train/handwaving",
         "person01_handwaving_d1_uncomp.avi",
         "handwaving"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actions_temp/train/running</td>\n",
       "      <td>person01_running_d3_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actions_temp/train/running</td>\n",
       "      <td>person01_running_d1_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actions_temp/train/running</td>\n",
       "      <td>person01_running_d2_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actions_temp/train/handwaving</td>\n",
       "      <td>person01_handwaving_d2_uncomp.avi</td>\n",
       "      <td>handwaving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actions_temp/train/handwaving</td>\n",
       "      <td>person01_handwaving_d1_uncomp.avi</td>\n",
       "      <td>handwaving</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          folder                          file_name  \\\n",
       "0     Actions_temp/train/running     person01_running_d3_uncomp.avi   \n",
       "1     Actions_temp/train/running     person01_running_d1_uncomp.avi   \n",
       "2     Actions_temp/train/running     person01_running_d2_uncomp.avi   \n",
       "3  Actions_temp/train/handwaving  person01_handwaving_d2_uncomp.avi   \n",
       "4  Actions_temp/train/handwaving  person01_handwaving_d1_uncomp.avi   \n",
       "\n",
       "        label  \n",
       "0     running  \n",
       "1     running  \n",
       "2     running  \n",
       "3  handwaving  \n",
       "4  handwaving  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.DataFrame()\n",
    "df_data = load_video_into_dataframe(data_dir)\n",
    "df_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "folder",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "file_name",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0750fb83-066e-4281-8210-a1f8b50729f3",
       "rows": [
        [
         "Handclapping",
         "3",
         "3"
        ],
        [
         "Walking",
         "3",
         "3"
        ],
        [
         "boxing",
         "3",
         "3"
        ],
        [
         "handwaving",
         "3",
         "3"
        ],
        [
         "jogging",
         "3",
         "3"
        ],
        [
         "running",
         "3",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Handclapping</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walking</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boxing</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handwaving</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jogging</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              folder  file_name\n",
       "label                          \n",
       "Handclapping       3          3\n",
       "Walking            3          3\n",
       "boxing             3          3\n",
       "handwaving         3          3\n",
       "jogging            3          3\n",
       "running            3          3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(\"label\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "folder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "file_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9eca6aea-925e-4fcb-a330-42c9a95bc1ac",
       "rows": [
        [
         "0",
         "Actions_temp/test/running",
         "person02_running_d2_uncomp.avi",
         "running"
        ],
        [
         "1",
         "Actions_temp/test/running",
         "person04_running_d4_uncomp.avi",
         "running"
        ],
        [
         "2",
         "Actions_temp/test/running",
         "person02_running_d1_uncomp.avi",
         "running"
        ],
        [
         "3",
         "Actions_temp/test/handwaving",
         "person02_handwaving_d1_uncomp.avi",
         "handwaving"
        ],
        [
         "4",
         "Actions_temp/test/handwaving",
         "person02_handwaving_d2_uncomp.avi",
         "handwaving"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actions_temp/test/running</td>\n",
       "      <td>person02_running_d2_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actions_temp/test/running</td>\n",
       "      <td>person04_running_d4_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actions_temp/test/running</td>\n",
       "      <td>person02_running_d1_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actions_temp/test/handwaving</td>\n",
       "      <td>person02_handwaving_d1_uncomp.avi</td>\n",
       "      <td>handwaving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actions_temp/test/handwaving</td>\n",
       "      <td>person02_handwaving_d2_uncomp.avi</td>\n",
       "      <td>handwaving</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         folder                          file_name       label\n",
       "0     Actions_temp/test/running     person02_running_d2_uncomp.avi     running\n",
       "1     Actions_temp/test/running     person04_running_d4_uncomp.avi     running\n",
       "2     Actions_temp/test/running     person02_running_d1_uncomp.avi     running\n",
       "3  Actions_temp/test/handwaving  person02_handwaving_d1_uncomp.avi  handwaving\n",
       "4  Actions_temp/test/handwaving  person02_handwaving_d2_uncomp.avi  handwaving"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data = pd.DataFrame()\n",
    "df_test_data = load_video_into_dataframe(test_data_dir)\n",
    "df_test_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "folder",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "file_name",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dc4a40d2-c6f0-43fd-b11b-92bd93552de7",
       "rows": [
        [
         "Handclapping",
         "3",
         "3"
        ],
        [
         "Walking",
         "3",
         "3"
        ],
        [
         "boxing",
         "3",
         "3"
        ],
        [
         "handwaving",
         "3",
         "3"
        ],
        [
         "jogging",
         "3",
         "3"
        ],
        [
         "running",
         "3",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Handclapping</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walking</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boxing</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handwaving</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jogging</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              folder  file_name\n",
       "label                          \n",
       "Handclapping       3          3\n",
       "Walking            3          3\n",
       "boxing             3          3\n",
       "handwaving         3          3\n",
       "jogging            3          3\n",
       "running            3          3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data.groupby(\"label\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocess_folder(name_preprocessed_folder, org_root_dir, datatype_dir):\n",
    "    datatype_dir_path = os.path.join(name_preprocessed_folder, datatype_dir)\n",
    "    if not os.path.exists(datatype_dir_path):\n",
    "        os.makedirs(datatype_dir_path)\n",
    "\n",
    "    # Loop through each subfolder in the data directory\n",
    "    for subfolder in os.listdir(org_root_dir):\n",
    "        class_dir_path = os.path.join(datatype_dir_path, subfolder)\n",
    "        if not os.path.exists(class_dir_path):\n",
    "            os.makedirs(class_dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_preprocess_folder(\"preprocessed_videos\", data_dir, \"train\")\n",
    "create_preprocess_folder(\"preprocessed_videos\", test_data_dir, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_avi(\n",
    "    video_path, output_folder, frame_prefix=\"frame_\", file_format=\".jpg\"\n",
    "):\n",
    "    try:\n",
    "        # Check if video file exists\n",
    "        if not os.path.isfile(video_path):\n",
    "            print(f\"Error: Video file '{video_path}' does not exist.\")\n",
    "            return -1\n",
    "\n",
    "        # Create output folder if it doesn't exist\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            print(f\"Created output directory: {output_folder}\")\n",
    "\n",
    "        # Initialize video capture\n",
    "        cap = cv.VideoCapture(video_path)\n",
    "\n",
    "        # Check if video opened successfully\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video file '{video_path}'.\")\n",
    "            return -1\n",
    "\n",
    "        # Get video properties\n",
    "        fps = cap.get(cv.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "        frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "        ret, frame = cap.read()\n",
    "        channels = frame.shape[2] if ret else None\n",
    "\n",
    "        print(\"Video properties:\")\n",
    "        print(f\"- FPS: {fps}\")\n",
    "        print(f\"- Total frames: {frame_count}\")\n",
    "        print(f\"- Duration: {duration:.2f} seconds\")\n",
    "        print(f\"- Height: {frame_height} \")\n",
    "        print(f\"- Width: {frame_width} \")\n",
    "        print(f\"- Channels: {channels} \")\n",
    "\n",
    "        # Reset to beginning of video\n",
    "        cap.set(cv.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "        # Extract frames\n",
    "        frame_number = 0\n",
    "        frames_saved = 0\n",
    "\n",
    "        while True:\n",
    "            # Read next frame\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            # Break the loop if we've reached the end of the video\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # Construct output filename with leading zeros for proper sorting\n",
    "            frame_filename = f\"{frame_prefix}{frame_number:06d}{file_format}\"\n",
    "            output_path = os.path.join(output_folder, frame_filename)\n",
    "\n",
    "            # Save the frame\n",
    "            cv.imwrite(output_path, frame)\n",
    "            frames_saved += 1\n",
    "\n",
    "            # Increment frame counter\n",
    "            frame_number += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "\n",
    "    print(f\"Extraction complete. Saved {frames_saved} frames to {output_folder}\")\n",
    "    return frames_saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "folder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "file_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "77267779-1565-458c-b184-4cb8931bb1c5",
       "rows": [
        [
         "0",
         "Actions_temp/train/running",
         "person01_running_d3_uncomp.avi",
         "running"
        ],
        [
         "1",
         "Actions_temp/train/running",
         "person01_running_d1_uncomp.avi",
         "running"
        ],
        [
         "2",
         "Actions_temp/train/running",
         "person01_running_d2_uncomp.avi",
         "running"
        ],
        [
         "3",
         "Actions_temp/train/handwaving",
         "person01_handwaving_d2_uncomp.avi",
         "handwaving"
        ],
        [
         "4",
         "Actions_temp/train/handwaving",
         "person01_handwaving_d1_uncomp.avi",
         "handwaving"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actions_temp/train/running</td>\n",
       "      <td>person01_running_d3_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actions_temp/train/running</td>\n",
       "      <td>person01_running_d1_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actions_temp/train/running</td>\n",
       "      <td>person01_running_d2_uncomp.avi</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actions_temp/train/handwaving</td>\n",
       "      <td>person01_handwaving_d2_uncomp.avi</td>\n",
       "      <td>handwaving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actions_temp/train/handwaving</td>\n",
       "      <td>person01_handwaving_d1_uncomp.avi</td>\n",
       "      <td>handwaving</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          folder                          file_name  \\\n",
       "0     Actions_temp/train/running     person01_running_d3_uncomp.avi   \n",
       "1     Actions_temp/train/running     person01_running_d1_uncomp.avi   \n",
       "2     Actions_temp/train/running     person01_running_d2_uncomp.avi   \n",
       "3  Actions_temp/train/handwaving  person01_handwaving_d2_uncomp.avi   \n",
       "4  Actions_temp/train/handwaving  person01_handwaving_d1_uncomp.avi   \n",
       "\n",
       "        label  \n",
       "0     running  \n",
       "1     running  \n",
       "2     running  \n",
       "3  handwaving  \n",
       "4  handwaving  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_videos(df, preprocessed_folder):\n",
    "    # Loop through each row in the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Get the file path\n",
    "        file_path = os.path.join(row[\"folder\"], row[\"file_name\"])\n",
    "\n",
    "        # Get the label\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        # Create a folder for the label if it doesn't exist\n",
    "        label_folder = os.path.join(\n",
    "            os.path.join(preprocessed_folder, label), row[\"file_name\"].split(\".\")[0]\n",
    "        )\n",
    "        if not os.path.exists(label_folder):\n",
    "            os.makedirs(label_folder)\n",
    "\n",
    "        # Extract frames from the video\n",
    "\n",
    "        number_of_frames_saved = extract_frames_from_avi(\n",
    "            video_path=file_path, output_folder=label_folder\n",
    "        )\n",
    "        print(f\"Extracted {number_of_frames_saved} frames in '{label_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 350\n",
      "- Duration: 14.00 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 350 frames to preprocessed_videos/train/running/person01_running_d3_uncomp\n",
      "Extracted 350 frames in 'preprocessed_videos/train/running/person01_running_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 335\n",
      "- Duration: 13.40 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 335 frames to preprocessed_videos/train/running/person01_running_d1_uncomp\n",
      "Extracted 335 frames in 'preprocessed_videos/train/running/person01_running_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 365\n",
      "- Duration: 14.60 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 365 frames to preprocessed_videos/train/running/person01_running_d2_uncomp\n",
      "Extracted 365 frames in 'preprocessed_videos/train/running/person01_running_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 656\n",
      "- Duration: 26.24 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 656 frames to preprocessed_videos/train/handwaving/person01_handwaving_d2_uncomp\n",
      "Extracted 656 frames in 'preprocessed_videos/train/handwaving/person01_handwaving_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 468\n",
      "- Duration: 18.72 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 468 frames to preprocessed_videos/train/handwaving/person01_handwaving_d1_uncomp\n",
      "Extracted 468 frames in 'preprocessed_videos/train/handwaving/person01_handwaving_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 456\n",
      "- Duration: 18.24 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 456 frames to preprocessed_videos/train/handwaving/person01_handwaving_d3_uncomp\n",
      "Extracted 456 frames in 'preprocessed_videos/train/handwaving/person01_handwaving_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 950\n",
      "- Duration: 38.00 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 950 frames to preprocessed_videos/train/Walking/person01_walking_d3_uncomp\n",
      "Extracted 950 frames in 'preprocessed_videos/train/Walking/person01_walking_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 555\n",
      "- Duration: 22.20 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 555 frames to preprocessed_videos/train/Walking/person01_walking_d1_uncomp\n",
      "Extracted 555 frames in 'preprocessed_videos/train/Walking/person01_walking_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 675\n",
      "- Duration: 27.00 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 675 frames to preprocessed_videos/train/Walking/person01_walking_d2_uncomp\n",
      "Extracted 675 frames in 'preprocessed_videos/train/Walking/person01_walking_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 405\n",
      "- Duration: 16.20 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 405 frames to preprocessed_videos/train/jogging/person01_jogging_d3_uncomp\n",
      "Extracted 405 frames in 'preprocessed_videos/train/jogging/person01_jogging_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 415\n",
      "- Duration: 16.60 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 415 frames to preprocessed_videos/train/jogging/person01_jogging_d1_uncomp\n",
      "Extracted 415 frames in 'preprocessed_videos/train/jogging/person01_jogging_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 425\n",
      "- Duration: 17.00 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 425 frames to preprocessed_videos/train/jogging/person01_jogging_d2_uncomp\n",
      "Extracted 425 frames in 'preprocessed_videos/train/jogging/person01_jogging_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 390\n",
      "- Duration: 15.60 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 390 frames to preprocessed_videos/train/boxing/person01_boxing_d2_uncomp\n",
      "Extracted 390 frames in 'preprocessed_videos/train/boxing/person01_boxing_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 465\n",
      "- Duration: 18.60 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 465 frames to preprocessed_videos/train/boxing/person01_boxing_d3_uncomp\n",
      "Extracted 465 frames in 'preprocessed_videos/train/boxing/person01_boxing_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 360\n",
      "- Duration: 14.40 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 360 frames to preprocessed_videos/train/boxing/person01_boxing_d1_uncomp\n",
      "Extracted 360 frames in 'preprocessed_videos/train/boxing/person01_boxing_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 428\n",
      "- Duration: 17.12 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 428 frames to preprocessed_videos/train/Handclapping/person01_handclapping_d3_uncomp\n",
      "Extracted 428 frames in 'preprocessed_videos/train/Handclapping/person01_handclapping_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 378\n",
      "- Duration: 15.12 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 378 frames to preprocessed_videos/train/Handclapping/person01_handclapping_d1_uncomp\n",
      "Extracted 378 frames in 'preprocessed_videos/train/Handclapping/person01_handclapping_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 550\n",
      "- Duration: 22.00 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 550 frames to preprocessed_videos/train/Handclapping/person01_handclapping_d2_uncomp\n",
      "Extracted 550 frames in 'preprocessed_videos/train/Handclapping/person01_handclapping_d2_uncomp'\n"
     ]
    }
   ],
   "source": [
    "extract_frames_from_videos(df_data, \"preprocessed_videos/train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 1492\n",
      "- Duration: 59.68 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 1492 frames to preprocessed_videos/test/running/person02_running_d2_uncomp\n",
      "Extracted 1492 frames in 'preprocessed_videos/test/running/person02_running_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 230\n",
      "- Duration: 9.20 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 230 frames to preprocessed_videos/test/running/person04_running_d4_uncomp\n",
      "Extracted 230 frames in 'preprocessed_videos/test/running/person04_running_d4_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 314\n",
      "- Duration: 12.56 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 314 frames to preprocessed_videos/test/running/person02_running_d1_uncomp\n",
      "Extracted 314 frames in 'preprocessed_videos/test/running/person02_running_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 550\n",
      "- Duration: 22.00 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 550 frames to preprocessed_videos/test/handwaving/person02_handwaving_d1_uncomp\n",
      "Extracted 550 frames in 'preprocessed_videos/test/handwaving/person02_handwaving_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 440\n",
      "- Duration: 17.60 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 440 frames to preprocessed_videos/test/handwaving/person02_handwaving_d2_uncomp\n",
      "Extracted 440 frames in 'preprocessed_videos/test/handwaving/person02_handwaving_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 460\n",
      "- Duration: 18.40 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 460 frames to preprocessed_videos/test/handwaving/person04_handwaving_d4_uncomp\n",
      "Extracted 460 frames in 'preprocessed_videos/test/handwaving/person04_handwaving_d4_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 535\n",
      "- Duration: 21.40 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 535 frames to preprocessed_videos/test/Walking/person05_walking_d1_uncomp\n",
      "Extracted 535 frames in 'preprocessed_videos/test/Walking/person05_walking_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 418\n",
      "- Duration: 16.72 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 418 frames to preprocessed_videos/test/Walking/person04_walking_d4_uncomp\n",
      "Extracted 418 frames in 'preprocessed_videos/test/Walking/person04_walking_d4_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 480\n",
      "- Duration: 19.20 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 480 frames to preprocessed_videos/test/Walking/person04_walking_d3_uncomp\n",
      "Extracted 480 frames in 'preprocessed_videos/test/Walking/person04_walking_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 570\n",
      "- Duration: 22.80 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 570 frames to preprocessed_videos/test/jogging/person07_jogging_d4_uncomp\n",
      "Extracted 570 frames in 'preprocessed_videos/test/jogging/person07_jogging_d4_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 312\n",
      "- Duration: 12.48 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 312 frames to preprocessed_videos/test/jogging/person07_jogging_d3_uncomp\n",
      "Extracted 312 frames in 'preprocessed_videos/test/jogging/person07_jogging_d3_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 456\n",
      "- Duration: 18.24 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 456 frames to preprocessed_videos/test/jogging/person07_jogging_d2_uncomp\n",
      "Extracted 456 frames in 'preprocessed_videos/test/jogging/person07_jogging_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 522\n",
      "- Duration: 20.88 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 522 frames to preprocessed_videos/test/boxing/person02_boxing_d4_uncomp\n",
      "Extracted 522 frames in 'preprocessed_videos/test/boxing/person02_boxing_d4_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 442\n",
      "- Duration: 17.68 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 442 frames to preprocessed_videos/test/boxing/person03_boxing_d1_uncomp\n",
      "Extracted 442 frames in 'preprocessed_videos/test/boxing/person03_boxing_d1_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 502\n",
      "- Duration: 20.08 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 502 frames to preprocessed_videos/test/boxing/person03_boxing_d2_uncomp\n",
      "Extracted 502 frames in 'preprocessed_videos/test/boxing/person03_boxing_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 372\n",
      "- Duration: 14.88 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 372 frames to preprocessed_videos/test/Handclapping/person06_handclapping_d4_uncomp\n",
      "Extracted 372 frames in 'preprocessed_videos/test/Handclapping/person06_handclapping_d4_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 386\n",
      "- Duration: 15.44 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 386 frames to preprocessed_videos/test/Handclapping/person08_handclapping_d2_uncomp\n",
      "Extracted 386 frames in 'preprocessed_videos/test/Handclapping/person08_handclapping_d2_uncomp'\n",
      "Video properties:\n",
      "- FPS: 25.0\n",
      "- Total frames: 442\n",
      "- Duration: 17.68 seconds\n",
      "- Height: 120 \n",
      "- Width: 160 \n",
      "- Channels: 3 \n",
      "Extraction complete. Saved 442 frames to preprocessed_videos/test/Handclapping/person08_handclapping_d3_uncomp\n",
      "Extracted 442 frames in 'preprocessed_videos/test/Handclapping/person08_handclapping_d3_uncomp'\n"
     ]
    }
   ],
   "source": [
    "extract_frames_from_videos(df_test_data, \"preprocessed_videos/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :], verbose=0,\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier/ckpt.weights.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, sequence_model = run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytc8suTIMN8y"
   },
   "source": [
    "#### Visualize the frames and analyze the object in each frame. (1 point)\n",
    "\n",
    "* Plot the frames of each class per row (6 rows)\n",
    "* Plot the title as label on each subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPQf1AoPMVFM"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QleRUjGgtwTP"
   },
   "source": [
    "### Create the Neural Network (4 points)\n",
    "\n",
    "We can build the model in several ways. We can use a well-known model that we inject in time distributed layer, or we can build our own.\n",
    "\n",
    "With custom ConvNet each input image of the sequence must pass to a convolutional network. The goal is to train that model for each frame and then decide the class to infer.\n",
    "\n",
    "* Use ConvNet and Time distributed to detect features.\n",
    "* Inject the Time distributed output to GRU or LSTM to treat as a time series.\n",
    "* Apply a DenseNet to take the decision and classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVY2FVVDnATG"
   },
   "source": [
    "##### Build the ConvNet for the feature extraction, GRU LSTM layers as a time series and Dense layers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1kHGAqLUlB0"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcYlSojgwIrb"
   },
   "source": [
    "#### Setup the parameters and train the model with epochs, batch wise\n",
    "\n",
    "* Use train data to fit the model and test data for validation\n",
    "* Configure batch size and epochs\n",
    "* Plot the loss of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAKAXQ1PVCe_"
   },
   "outputs": [],
   "source": [
    "# Note: There will be a high memory requirement for the training steps below.\n",
    "# You should work on a GPU/TPU based runtime. See 'Change Runtime' in Colab\n",
    "# Training time for each epoch could be ~30 mins\n",
    "# To save and re-load your model later, see the reference below:\n",
    "# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuIG5DHUOAkm"
   },
   "source": [
    "### Use pre-trained model for feature extraction (3 points)\n",
    "\n",
    "To create a deep learning network for video classification:\n",
    "\n",
    "* Convert videos to sequences of feature vectors using a pretrained convolutional neural network, such as VGG16, to extract features from each frame.\n",
    "\n",
    "* Train an LSTM network on the sequences to predict the video labels.\n",
    "\n",
    "* Assemble a network that classifies videos directly by combining layers from both networks.\n",
    "\n",
    "Hint: [VGG-16 CNN and LSTM](https://riptutorial.com/keras/example/29812/vgg-16-cnn-and-lstm-for-video-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqCpO1LO_4S1"
   },
   "source": [
    "#### Load and fine-tune the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhFb7AeA_2rg"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBMJM0U9_00u"
   },
   "source": [
    "#### Setup the parameters and train the model with epochs, batch wise\n",
    "\n",
    "* Use train data to fit the model and test data for validation\n",
    "* Configure batch size and epochs\n",
    "* Plot the loss of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpJlYHcIBuyk"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owOW-XyZb792"
   },
   "source": [
    "### Report Analysis\n",
    "\n",
    "* Discuss on FPS, Number of frames and duration of each video\n",
    "* Analyze the impact of the LSTM, GRU and TimeDistributed layers\n",
    "* Discuss about the model convergence using pre-trained and ConvNet\n",
    "* *Additional Reading*: Read and discuss about the use of Conv3D in video classification"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
