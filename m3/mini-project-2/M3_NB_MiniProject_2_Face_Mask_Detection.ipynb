{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"another-optimum"},"source":["# Advanced Certification Program in Computational Data Science\n","\n","##  A program by IISc and TalentSprint\n","\n","### Mini Project Notebook: Face Mask Detection using Convolutional Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"maritime-miami"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"nljJR6CwfZN_"},"source":["At the end of the miniproject, you will be able to :\n","\n","* load and extract features of images using ImageDataGenerator\n","\n","* build the convolutional neural networks\n","\n","* use the pre-trained models using keras applications"]},{"cell_type":"markdown","metadata":{"id":"29152de7"},"source":["## Introduction\n","\n","This project uses a Deep Neural Network, more specifically a Convolutional Neural Network, to differentiate between images of people, with masks, without masks and incorrectly placed masks. Manually built and pretrained networks will be used to perform this classification task.\n","\n","**Face-Mask-Detection-Using-CNN**\n","\n","* Outbreak of the Coronavirus pandemic has created various changes in the lifestyle of everyone around the world.\n","* Among these changes, wearing a mask has been very vital to every individual.\n","* Detection of people who are not wearing masks is a challenge due to the large populations.\n","* This face mask detection project can be used in schools, hospitals, banks, airports etc as a digitalized scanning tool.\n","  - The technique of detecting peopleâ€™s faces and segregating them into three classes namely the people with masks and people without masks and partial masks is done with the help of image processing and deep learning.\n","* With the help of this project, a person who is monitoring the face mask status for a particular firm can be seated in a remote area and still monitor efficiently and give instructions accordingly.\n","\n","![img](https://cdn.iisc.talentsprint.com/CDS/MiniProjects/dataset-images-with-mask.jpg)"]},{"cell_type":"markdown","metadata":{"id":"surprising-uruguay"},"source":["## Dataset\n","\n","The data for this mini-project is collected from various sources including the masked images from internet and general frontal face images considered as without mask. This dataset consists of 5029 train images and 1259 test images with 3 classes `with_mask`, `without_mask` and `partial_mask`\n","\n","Many people do not correctly wear their masks due to bad practices, bad behaviors or vulnerability of individuals (e.g., children, old people). For these reasons, several mask wearing campaigns intend to sensitize people about this problem and good practices. In this sense, this work proposes three types of masked face detection dataset:\n","\n","- Without Mask/ With Mask/ Partial Mask\n","\n","Note that this dataset contains some annotated (artificially generated) masks to augment the 'masked' data category."]},{"cell_type":"markdown","metadata":{"id":"ih-oasWmdZul"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"qfWGmjNHdZul"},"source":["To build and implement a Convolutional Neural Network model to classify between masked/unmasked/partially masked faces."]},{"cell_type":"markdown","metadata":{"id":"operating-latter"},"source":["## Grading = 10 Points"]},{"cell_type":"code","metadata":{"id":"812a816f","cellView":"form"},"source":["#@title Download the data\n","!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/MP2_FaceMask_Dataset.zip\n","!unzip -qq MP2_FaceMask_Dataset.zip\n","print(\"Data Downloaded Successfuly!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"abstract-stocks"},"source":["### Import Required packages"]},{"cell_type":"code","metadata":{"id":"YG52PDGENRgN"},"source":["from tensorflow.keras.optimizers import RMSprop\n","#from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n","from keras.models import Model, load_model\n","from keras.callbacks import TensorBoard, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.utils import shuffle\n","import numpy as np\n","import pandas as pd\n","import PIL\n","from matplotlib import pyplot as plt\n","from keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow import keras\n","import glob, os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"53g0zVbjRV7K"},"source":["## Data Loading and preprocessing (2 points)"]},{"cell_type":"markdown","metadata":{"id":"aYSjwlcSGJq1"},"source":["### Analyze the shape of images and distribution of classes"]},{"cell_type":"code","metadata":{"id":"Z_FC0knCfeFD"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HN1ZYJL8Ts0Q"},"source":["### Load the images using ImageDataGenerator\n","\n","There are two main steps involved in creating the generator.\n","1. Instantiate ImageDataGenerator with required arguments to create an object\n","2. Use the `flow_from_directory` command depending on how your data is stored on disk. This is the command that will allow you to generate and get access to batches of data on the fly.\n","\n","Hint: [link](https://keras.io/api/preprocessing/image/)"]},{"cell_type":"code","metadata":{"id":"LqsC5w4f5NOr"},"source":["TRAINING_DIR = \"/content/MP2_FaceMask_Dataset/train/\"\n","VALIDATION_DIR = \"/content/MP2_FaceMask_Dataset/test/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBmLkk-zNeZs"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vdQY-dzZeqWm"},"source":["### Visualize the sample images of each class using data generator\n","\n","Hint: plt.subplot"]},{"cell_type":"code","metadata":{"id":"HCgAcqdGewsb"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkOr1nhNT5yD"},"source":["## Build the CNN model using Keras (4 points)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x1GGjQTfTIoO"},"source":["**Convolutional Neural Network:** A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:\n","\n","* convolutional layers\n","* pooling layers\n","* dense layers\n","\n","\n","**Conv2D**\n","\n","Passing an image with input shape of 3-D and to calculate the output:\n","\n"," $O = \\frac{n - f + 2p}{s} + 1$\n","\n"," where\n","\n"," $n$ = image dimension\n","\n"," $f$ = filter size\n","\n"," $p$ = padding\n","\n"," $s$ = stride\n","\n","**MaxPool**\n","\n","The resulting output, when using the \"valid\" padding option, has a spatial shape (number of rows or columns) of:\n","\n","O = `math.floor`$(\\frac{input shape - pool size)}{ strides}) + 1$ (when input shape >= pool size)\n","\n","The resulting output shape when using the \"same\" padding option is:\n","\n","O = `math.floor`$(\\frac{input shape - 1}{strides}) + 1$\n","\n","by default, stride = None, so stride is same as pool size"]},{"cell_type":"markdown","metadata":{"id":"PmGNu31vmLhs"},"source":["Task-flow\n","* Initialize the network of convolution, maxpooling and dense layers\n","* Define the optimizer and loss functions\n","* Fit the model and evaluate the model"]},{"cell_type":"code","metadata":{"id":"TdaobFneNaJO"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wFPKELTRnv9s"},"source":["## Transfer learning (4 points)\n","\n","Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.\n","\n","A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task.\n","\n","The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n","\n","For eg. Using VGG16, we remove the last layer which takes a probability for each of the 1000 classes in the ImageNet and replaces it with a layer that takes 3 probabilities in our case."]},{"cell_type":"markdown","metadata":{"id":"0lMUVOj0UdAi"},"source":["### Use the pre-trained models ([VGG16](https://cdn.exec.talentsprint.com/static/cds/content/M6_SNB_MiniProject_2_VGG16.pdf) or [ResNet50](https://cdn.exec.talentsprint.com/static/cds/content/M6_SNB_MiniProject_2_ResNet50.pdf))\n","\n","* Load the pre-trained model\n","* Fit and evaluate the data\n","\n","Hint: [How to use pre-trained model](https://towardsdatascience.com/how-to-use-a-pre-trained-model-vgg-for-image-classification-8dd7c4a4a517/)"]},{"cell_type":"markdown","metadata":{"id":"7YI0gnq-DLyu"},"source":["#### Expected accuracy: More than 90%"]},{"cell_type":"markdown","metadata":{"id":"Y4GX9CwFqOXm"},"source":["Task-flow\n","* Initialize the network with the weights of Imagenet\n","* Fine tune the network by modifying fully connected layers.\n","* Re-train the model with our problem data"]},{"cell_type":"markdown","source":["#### VGG16"],"metadata":{"id":"4hNQpn2BV0KE"}},{"cell_type":"code","source":["# Create a custom image classification model using VGG16 as the base with added Dense layers for fine-tuning in TensorFlow Keras.\n","# YOUR CODE HERE"],"metadata":{"id":"tunC8hMmVov7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile the VGG16-based model with the Adam optimizer (learning rate 0.00001), categorical cross-entropy loss, and accuracy as the metric\n","# YOUR CODE HERE"],"metadata":{"id":"o0azzpcJXfmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the VGG16-based model for 5 epochs using the training and validation data generators, with specified class weights {0:1.0, 1:0.8, 2:1.2}\n","# YOUR CODE HERE"],"metadata":{"id":"JUVSKkQOXjnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the training and validation loss over epochs\n","# YOUR CODE HERE"],"metadata":{"id":"os34VO-3X8bI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the model in HDF5 format\n","# YOUR CODE HERE"],"metadata":{"id":"v5vSC_iXYIiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the model in keras format\n","# YOUR CODE HERE"],"metadata":{"id":"uwAYw3wwYPL2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ResNet50"],"metadata":{"id":"M-IUkn5GV1gd"}},{"cell_type":"code","metadata":{"id":"nx2Z2vLZqXm0"},"source":["# Build and compile a sequential model using ResNet50 as the base, with added Dense layers for classification, and display the model summary\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the ResNet50-based model for 5 epochs using the training and validation data generators\n","# YOUR CODE HERE"],"metadata":{"id":"9mjJDKkEYhZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the training and validation loss over epochs\n","# YOUR CODE HERE"],"metadata":{"id":"wuUSFdqVYtIt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4tF71VIUwzK"},"source":["### Capture the live image using the below code cell and predict"]},{"cell_type":"code","metadata":{"cellView":"form","id":"I6kTLnTmrcCg"},"source":["#@title Capture the photo\n","from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename\n","\n","from IPython.display import Image\n","try:\n","  filename = take_photo()\n","  print('Saved to {}'.format(filename))\n","  display(Image(filename))\n","except Exception as err:\n","  print(str(err))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rNe9vDQ6q6Z6"},"source":["After executing above cell and capturing the photo, load the captured photo and predict with model.\n","\n","**Note:**\n","* Convert the image to numpy array and resize to the shape which the model accepts.\n","* Extend the dimension (to 4-D shape) of an image, as the model is trained on a batch of inputs."]},{"cell_type":"code","metadata":{"id":"XD5gr9YOAX83"},"source":["MODEL = model_vgg\n","features = PIL.Image.open(\"photo.jpg\")\n","features = features.resize((224, 224))\n","plt.imshow(features);\n","# YOUR CODE HERE to predict the image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Report Analysis\n","\n","- Compare the accuracies for the Pre-trained vs CNN models\n","- Which model detects the mask/no mask/ partial mask more accurately with the live pictures?\n","- What process was followed to tune the hyperparameters?\n","- Discuss the confusion matrix in terms of the misclassifications"],"metadata":{"id":"kCoEGL0FZc2c"}},{"cell_type":"markdown","source":["### Kaggle predictions"],"metadata":{"id":"PFwBCd8CZiJR"}},{"cell_type":"code","source":["# Download the saved VGG model\n","# Your code here"],"metadata":{"id":"JOeQ9QlFaqPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKnc1WZk9cIk"},"source":["### Instructions for preparing Kaggle competition predictions\n","\n","Important: Refer the [Kaggle Team Creation Instructions Doc](https://drive.google.com/file/d/1eB26tqAXrP3z1WTjSEbOnaSqAOqdWJ7O/view?usp=drive_link)\n","\n","* Load the image paths from Kaggle testset using `glob`\n","* Read the Images using `PIL.Image.open` and resize to required shape.\n","* Get the predictions using trained model and prepare a csv file\n","  - FC layer of DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n","* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n","  - First column is the img_path which is considered as index\n","  - Second column is prediction in decoded form (for eg. with_mask, partial_mask, without_mask).\n","  - Note that at the time of submission to Kaggle, predictions need to be sorted in ascending order i.e, same as sample submission file.\n","\n","<font color='magenta'>If you are unable to download the data and sample submission from the Kaggle Competition site then please use the download cell below.</font>\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"SsPbLWWqimq9"},"source":["#@title Download test data and SampleSubmission file for Kaggle submission\n","!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/FaceMask_Kaggle_test.zip\n","!unzip -qq FaceMask_Kaggle_test.zip\n","!wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/Sample_Submission.csv\n","print(\"Data Downloaded Successfuly!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Create a dataframe with kaggle data"],"metadata":{"id":"wL0Ndx4NbvJg"}},{"cell_type":"code","source":["# Create a DataFrame of image paths from the \"FaceMask_Kaggle_test\" directory, extract and sort by the numeric order in the filenames, and display the DataFrame\n","# YOUR CODE HERE"],"metadata":{"id":"ToQ4ocw3bweB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display rows 120 to 135 of the DataFrame\n","# YOUR CODE HERE"],"metadata":{"id":"ttzQJe0GcFiN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Extract features of kaggle data"],"metadata":{"id":"NvYQdJ-bcMHd"}},{"cell_type":"code","source":["# Resize images from the img_path column in the DataFrame df to 224x224, append them to a list, and convert the list to a NumPy array\n","# YOUR CODE HERE"],"metadata":{"id":"7I9xrYzlcULO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Load the saved model and predict the kaggle features"],"metadata":{"id":"WDjVmsCJce41"}},{"cell_type":"code","source":["# Ensure consistent usage of TensorFlow's Keras\n","# YOUR CODE HERE"],"metadata":{"id":"t7l37oXvcv-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate predictions\n","# YOUR CODE HERE"],"metadata":{"id":"PfMlIVplc06V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the model predictions to class labels using argmax and display the unique predicted classes.\n","# YOUR CODE HERE"],"metadata":{"id":"w_txPkXcdGBF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Prepare the kaggle predictions CSV file after decoding the integer predictions"],"metadata":{"id":"vmiT7_NsdLBf"}},{"cell_type":"code","source":["# Create a DataFrame with image paths and predicted labels, replacing numeric labels with corresponding mask status categories.\n","# YOUR CODE HERE"],"metadata":{"id":"JW0nk5fydUY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the kaggle DataFrame to a CSV file named \"kaggle_submission_w2.csv\" without including the index.\n","# YOUR CODE HERE"],"metadata":{"id":"sGjkAdRjdadD"},"execution_count":null,"outputs":[]}]}