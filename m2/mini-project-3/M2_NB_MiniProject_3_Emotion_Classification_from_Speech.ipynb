{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"associate-sunset"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Mini-Project: Speech Emotion Classification"],"id":"associate-sunset"},{"cell_type":"markdown","metadata":{"id":"handled-tooth"},"source":["## Problem Statement"],"id":"handled-tooth"},{"cell_type":"markdown","metadata":{"id":"accessory-watts"},"source":["Build a model to recognize emotion from speech using Ensemble learning"],"id":"accessory-watts"},{"cell_type":"markdown","metadata":{"id":"twenty-indonesia"},"source":["## Learning Objectives"],"id":"twenty-indonesia"},{"cell_type":"markdown","metadata":{"id":"honest-friendship"},"source":["At the end of the mini-project, you will be able to :\n","\n","* extract the features from audio data\n","* implement ML classification algorithms individually and as Ensembles, to classify emotions\n","* record the voice sample and test it with trained model"],"id":"honest-friendship"},{"cell_type":"markdown","metadata":{"id":"lesbian-bottom"},"source":["## Dataset"],"id":"lesbian-bottom"},{"cell_type":"markdown","metadata":{"id":"fixed-trainer"},"source":["**TESS Dataset**\n","\n","The first dataset chosen for this mini-project is the [TESS](https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP2/E8H2MF) (Toronto emotional speech set) dataset. It contains 2880 files.  A set of 200 target words were spoken in the carrier phrase \"Say the word _____' by two actresses and the sets were recorded in seven different emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). Both actresses spoke English as their first language, were university educated, and had musical training. Audiometric testing indicated that both actresses had thresholds within the normal range."],"id":"fixed-trainer"},{"cell_type":"markdown","metadata":{"id":"Roo5A2aLVI07"},"source":["**Ravdess Dataset**\n","\n","The second dataset chosen for this mini-project is [Ravdess](https://zenodo.org/record/1188976#.YLczy4XivIU) (The Ryerson Audio-Visual Database of Emotional Speech and Song). This dataset contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.\n","\n","**File naming convention**\n","\n","Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n","\n","**Filename identifiers**\n","\n","* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n","* Vocal channel (01 = speech, 02 = song).\n","* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n","* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n","* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n","* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n","* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n","\n","Filename example: `03-01-06-01-02-01-12.wav`\n","\n","    - Audio-only - 03\n","    - Speech - 01\n","    - Fearful - 06\n","    - Normal intensity - 01\n","    - Statement \"dogs\" - 02\n","    - 1st Repetition - 01\n","    - 12th Actor - 12 Female, as the actor ID number is even."],"id":"Roo5A2aLVI07"},{"cell_type":"markdown","metadata":{"id":"GIR6a6TvVnRY"},"source":["## Information"],"id":"GIR6a6TvVnRY"},{"cell_type":"markdown","metadata":{"id":"mediterranean-february"},"source":["**Speech Emotion Recognition (SER)** is the task of recognizing the emotion from  speech, irrespective of the semantics. Humans can efficiently perform this task as a natural part of speech communication, however, the ability to conduct it automatically using programmable devices is a field of active research.\n","\n","Studies of automatic emotion recognition systems aim to create efficient, real-time methods of detecting the emotions of mobile phone users, call center operators and customers, car drivers, pilots, and many other human-machine communication users. Adding emotions to machines forms an important aspect of making machines appear and act in a human-like manner\n","\n","Lets gain familiarity with some of the audio based features that are commonly used for SER.\n","\n","**Mel scale** — The mel scale (derived from the word *melody*) is a perceptual scale of pitches judged by listeners to be equal in distance from one another. The reference point between this scale and normal frequency measurement is defined by assigning a perceptual pitch of 1000 mels to a 1000 Hz tone, 40 dB above the listener's threshold. Above about 500 Hz, increasingly large intervals are judged by listeners to produce equal pitch increments. Refer [here](https://towardsdatascience.com/learning-from-audio-the-mel-scale-mel-spectrograms-and-mel-frequency-cepstral-coefficients-f5752b6324a8) for more detailed information.\n","\n","**Pitch** — how high or low a sound is. It depends on frequency, higher pitch is high frequency\n","\n","**Frequency** — speed of vibration of sound, measures wave cycles per second\n","\n","**Chroma** — Representation for audio where spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma). Computed by summing the log frequency magnitude spectrum across octaves.\n","\n","**Fourier Transforms** — used to convert from time domain to frequency domain. Time domain shows how signal changes over time. Frequency domain shows how much of the signal lies within each given frequency band over a range of frequencies"],"id":"mediterranean-february"},{"cell_type":"markdown","metadata":{"id":"Q5a6Dz9wCxOc"},"source":["**Librosa**\n","\n","[Librosa](https://librosa.org/doc/latest/index.html) is a Python package, built for speech and audio analytics. It provides modular functions that simplify working with audio data and help in achieving a wide range of applications such as identification of the personal characteristics of different individuals' voice samples, detecting emotions from audio samples etc.\n","\n","For further details on the Librosa package, refer [here](https://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf).\n"],"id":"Q5a6Dz9wCxOc"},{"cell_type":"markdown","source":["### **Kaggle Competition**"],"metadata":{"id":"3fjmaAe32Z_K"},"id":"3fjmaAe32Z_K"},{"cell_type":"markdown","source":["Please refer to the link for viewing the\n","[Kaggle Competition Document](https://drive.google.com/file/d/1M4LO6727OIpa4_IXeWQKmUbN9nMpfDDJ/view?usp=drive_link) and join the Kaggle Competition using the hyperlink given in this document under '*Kaggle* Competition site'.\n"],"metadata":{"id":"y4yw7Ewy09Ut"},"id":"y4yw7Ewy09Ut"},{"cell_type":"markdown","metadata":{"id":"operating-latter"},"source":["## Grading = 10 Points"],"id":"operating-latter"},{"cell_type":"code","metadata":{"id":"talented-upset","cellView":"form"},"source":["#@title Download the datasets and install packages\n","!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/Ravdess_Tess.zip\n","!unzip -qq Ravdess_Tess.zip\n","# Install packages\n","!pip -qq install librosa soundfile\n","!pip -qq install wavio\n","print(\"Datasets downloaded successfully!\")"],"id":"talented-upset","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"appreciated-pattern"},"source":["### Import Neccesary Packages"],"id":"appreciated-pattern"},{"cell_type":"code","metadata":{"id":"loose-marsh"},"source":["import librosa\n","import librosa.display\n","import soundfile\n","import os, glob, pickle\n","import numpy as np\n","import pandas as pd\n","import IPython.display as ipd\n","from matplotlib import pyplot as plt\n","from datetime import datetime\n","from IPython.display import Javascript\n","from google.colab import output\n","from base64 import b64decode\n","import warnings\n","warnings.filterwarnings('ignore')\n","# sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn import tree\n","from sklearn.ensemble import VotingClassifier"],"id":"loose-marsh","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2vxa85gvorY5"},"source":["### Work-Flow\n","\n","* Load the TESS audio data and extract features and labels\n","\n","* Load the Ravdess audio data and extract features\n","\n","* Combine both the audio dataset features\n","\n","* Train and test the model with TESS + Ravdess Data\n","\n","* Record the team audio samples and add them to TESS + Ravdess data\n","\n","* Train and test the model with TESS + Ravdess + Team Recorded (combined) data\n","\n","* Test each of the models with live audio sample recording."],"id":"2vxa85gvorY5"},{"cell_type":"markdown","metadata":{"id":"compressed-reflection"},"source":["### Load the Tess data and Ravdess data audio files (1 point)\n","\n","Hint: `glob.glob`"],"id":"compressed-reflection"},{"cell_type":"code","metadata":{"id":"518e945a"},"source":["# YOUR CODE HERE"],"id":"518e945a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sound-chest"},"source":["#### Play the sample audio"],"id":"sound-chest"},{"cell_type":"code","metadata":{"id":"personalized-wildlife"},"source":["# YOUR CODE HERE"],"id":"personalized-wildlife","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"exposed-county"},"source":["### Data Exploration and Visualization (1 point)"],"id":"exposed-county"},{"cell_type":"markdown","metadata":{"id":"hungry-cleaner"},"source":["#### Visualize the distribution of all the labels"],"id":"hungry-cleaner"},{"cell_type":"code","metadata":{"id":"orange-taiwan"},"source":["# YOUR CODE HERE"],"id":"orange-taiwan","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"established-airfare"},"source":["#### Visualize sample audio signal using librosa"],"id":"established-airfare"},{"cell_type":"code","metadata":{"id":"outstanding-caribbean"},"source":["# YOUR CODE HERE"],"id":"outstanding-caribbean","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"medical-confidence"},"source":["### Feature extraction (2 points)\n","\n","Read one WAV file at a time using `Librosa`. An audio time series in the form of a 1-dimensional array for mono or 2-dimensional array for stereo, along with time sampling rate (which defines the length of the array), where the elements within each of the arrays represent the amplitude of the sound waves is returned by `librosa.load()` function. Refer to the supplementary notebook ('Audio feature extraction')\n","\n","To know more about Librosa, explore the [link](https://librosa.org/doc/latest/feature.html)"],"id":"medical-confidence"},{"cell_type":"code","metadata":{"id":"piano-accent"},"source":["# YOUR CODE HERE"],"id":"piano-accent","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7d3bd640"},"source":["#### Create a dictionary or a function to encode the emotions"],"id":"7d3bd640"},{"cell_type":"code","metadata":{"id":"c0c8d44d"},"source":["# YOUR CODE HERE"],"id":"c0c8d44d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coupled-villa"},"source":["#### TESS data feature extraction"],"id":"coupled-villa"},{"cell_type":"code","metadata":{"id":"quarterly-adrian"},"source":["# YOUR CODE HERE"],"id":"quarterly-adrian","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKzclsG-FnW6"},"source":["#### Ravdess data feature extraction"],"id":"qKzclsG-FnW6"},{"cell_type":"code","metadata":{"id":"iDwNOgKEIH3w"},"source":["# YOUR CODE HERE"],"id":"iDwNOgKEIH3w","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bb62f16"},"source":["#### Save the features\n","\n","It is best advised to save the features in dataframe and maintain so that feature extraction step is not required to be performed every time.\n","\n","* Make a DataFrame with features and labels\n","\n","* Write dataframe into `.CSV` file and save it offline."],"id":"2bb62f16"},{"cell_type":"code","metadata":{"id":"9ec91c16"},"source":["# YOUR CODE HERE"],"id":"9ec91c16","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aggressive-cause"},"source":["#### Split the data into train and test"],"id":"aggressive-cause"},{"cell_type":"code","metadata":{"id":"nearby-angle"},"source":["# YOUR CODE HERE"],"id":"nearby-angle","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ordered-weapon"},"source":["### Train the model with TESS + Ravdess data (2 points)\n","\n","* Apply different ML algorithms (eg. DecisionTree, RandomForest, etc.) and find the model with best performance"],"id":"ordered-weapon"},{"cell_type":"code","metadata":{"id":"EtlfZmtWSD_X"},"source":["# YOUR CODE HERE"],"id":"EtlfZmtWSD_X","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"returning-bones"},"source":["#### Apply the voting classifier"],"id":"returning-bones"},{"cell_type":"code","metadata":{"id":"fuzzy-respondent"},"source":["# YOUR CODE HERE"],"id":"fuzzy-respondent","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6p1MHeY9oYqB"},"source":["### Train the model with TESS + Ravdess + Team recorded data (4 points)\n","\n","* Record the audio samples (team data), extract features and combine with TESS + Ravdess data features\n","  - Record and gather all the team data samples with proper naming convention in separate folder\n","\n","    **Hint:** Follow the supplementary notebook to record team data\n","\n","  - Each team member must record 2 samples for each emotion (Use similar sentences as given in TESS data)\n","\n","* Train the different ML algorithms and find the model with best performance"],"id":"6p1MHeY9oYqB"},{"cell_type":"markdown","metadata":{"id":"M4In88rvtVmb"},"source":["#### Load the team data"],"id":"M4In88rvtVmb"},{"cell_type":"code","metadata":{"id":"nna7EZMRaKCI"},"source":["# YOUR CODE HERE"],"id":"nna7EZMRaKCI","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awnyYAic36V0"},"source":["#### Extracting features of team data and combine with TESS + Ravdess"],"id":"awnyYAic36V0"},{"cell_type":"code","metadata":{"id":"15br8qPwa2a6"},"source":["# YOUR CODE HERE"],"id":"15br8qPwa2a6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-ulIdIdtkDB"},"source":["#### Train the different ML algorithms"],"id":"w-ulIdIdtkDB"},{"cell_type":"code","metadata":{"id":"xuUx7UTFtjIs"},"source":["# YOUR CODE HERE"],"id":"xuUx7UTFtjIs","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMl0sRhXtXzL"},"source":["#### Test the best working model with live audio recording"],"id":"iMl0sRhXtXzL"},{"cell_type":"code","metadata":{"id":"4znf9-pcTbUB"},"source":["# choose the best working model and assign below\n","MODEL ="],"id":"4znf9-pcTbUB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"2d598770"},"source":["#@title Speak the utterance and test\n","from IPython.display import Javascript\n","from google.colab import output\n","from base64 import b64decode\n","\n","RECORD = \"\"\"\n","const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n","const b2text = blob => new Promise(resolve => {\n","  const reader = new FileReader()\n","  reader.onloadend = e => resolve(e.srcElement.result)\n","  reader.readAsDataURL(blob)\n","})\n","var record = time => new Promise(async resolve => {\n","  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n","  recorder = new MediaRecorder(stream)\n","  chunks = []\n","  recorder.ondataavailable = e => chunks.push(e.data)\n","  recorder.start()\n","  await sleep(time)\n","  recorder.onstop = async ()=>{\n","    blob = new Blob(chunks)\n","    text = await b2text(blob)\n","    resolve(text)\n","  }\n","  recorder.stop()\n","})\n","\"\"\"\n","\n","if not os.path.exists('ModelTesting/'):\n","    os.mkdir(\"ModelTesting/\")\n","def record(sec=3):\n","    print(\"Start speaking!\")\n","    now = datetime.now()\n","    current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n","    display(Javascript(RECORD))\n","    s = output.eval_js('record(%d)' % (sec*1000))\n","    b = b64decode(s.split(',')[1])\n","    with open('ModelTesting/audio_'+current_time+'.wav','wb') as f:\n","        f.write(b)\n","    return 'ModelTesting/audio_'+current_time+'.wav'\n","test_i = record()\n","pred = MODEL.predict(extract_feature(test_i).reshape(1,-1))\n","idx_emotion = list(emotions.values()).index(pred[0])\n","print(list(emotions.keys())[idx_emotion])\n","ipd.Audio(test_i)"],"id":"2d598770","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7n58VaeWKGu"},"source":["### Report Analysis\n","\n","- Report the accuracy for 10 live samples using the model trained on TESS+Ravdess+Team data\n","- Discuss with the team mentor regarding deep learnt audio features. Read a related article [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805181).\n"],"id":"K7n58VaeWKGu"}]}