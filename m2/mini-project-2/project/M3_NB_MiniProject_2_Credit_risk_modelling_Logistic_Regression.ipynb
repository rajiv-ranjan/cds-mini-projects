{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "associate-sunset",
   "metadata": {
    "id": "associate-sunset"
   },
   "source": [
    "# Advanced Certification Program in Computational Data Science\n",
    "## A program by IISc and TalentSprint\n",
    "### Mini-Project: Credit risk modelling using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-tooth",
   "metadata": {
    "id": "handled-tooth"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-watts",
   "metadata": {
    "id": "accessory-watts"
   },
   "source": [
    "`Predict the loan defaulters` using a Logistic Regression model on the `credit risk data` and `calculate credit scores`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-indonesia",
   "metadata": {
    "id": "twenty-indonesia"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-friendship",
   "metadata": {
    "id": "honest-friendship"
   },
   "source": [
    "At the end of the mini-project, you will be able to :\n",
    "\n",
    "* perform data exploration, preprocessing and visualization\n",
    "* implement Logistic Regression using manual code or using sklearn library\n",
    "* evaluate the model using appropriate performance metrics\n",
    "* develop a credit scoring system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-bottom",
   "metadata": {
    "id": "lesbian-bottom"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-trainer",
   "metadata": {
    "id": "fixed-trainer"
   },
   "source": [
    "The dataset chosen for this mini-project is the [Give Me Some Credit](https://cdn.iisc.talentsprint.com/CDS/Give_me_some_credit_BigML.pdf) dataset which can be used to build models for predicting loan repayment defaulters\n",
    "#### Datafields\n",
    "\n",
    "- **SeriousDlqin2yrs:** Person experienced 90 days past due delinquency or worse\n",
    "- **RevolvingUtilizationOfUnsecuredLines:** Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits\n",
    "- **age:** Age of borrower in years\n",
    "- **NumberOfTime30-59DaysPastDueNotWorse:** Number of times borrower has been 30-59 days past due but no worse in the last 2 years.\n",
    "- **DebtRatio:** Monthly debt payments, alimony,living costs divided by monthy gross income\n",
    "- **MonthlyIncome:** Monthly income\n",
    "- **NumberOfOpenCreditLinesAndLoans:** Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)\n",
    "- **NumberOfTimes90DaysLate:** Number of times borrower has been 90 days or more past due.\n",
    "- **NumberRealEstateLoansOrLines:**\tNumber of mortgage and real estate loans including home equity lines of credit\n",
    "- **NumberOfTime60-89DaysPastDueNotWorse:**\tNumber of times borrower has been 60-89 days past due but no worse in the last 2 years.\n",
    "- **NumberOfDependents:** Number of dependents in family excluding themselves (spouse, children etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-hierarchy",
   "metadata": {
    "id": "rapid-hierarchy"
   },
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-matter",
   "metadata": {
    "id": "prescribed-matter"
   },
   "source": [
    "Credit risk arises when a corporate or individual borrower fails to meet their debt obligations. From the lender's perspective, credit risk could disrupt its cash flows or increase collection costs, since the lender may be forced to hire a debt collection agency to enforce the collection. The loss may be partial or complete, where the lender incurs a loss of part of the loan or the entire loan extended to the borrower.\n",
    "\n",
    "Credit scoring algorithms, which calculate the probability of default, are the best methods that banks use to determine whether or not a loan should be granted.\n",
    "\n",
    "In order to build a credit scoring system, the following feature transformations are performed:\n",
    "\n",
    "#### Weight of Evidence and Information value\n",
    "\n",
    "Logistic regression is a commonly used technique in credit scoring for solving binary classification problems. Prior to model fitting, another iteration of variable selection is valuable to check if the newly WOE transformed variables are still good model candidates. Preferred candidate variables are those with higher information value having a linear relationship with the dependent variable, have good coverage across all categories, have a normal distribution, contain a notable overall contribution, and are relevant to the business.\n",
    "\n",
    "**Weight of evidence** (WOE) is a powerful tool for feature representation and evaluation in data science. WOE can provide interpret able transformation to both categorical and numerical features. The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. \"Bad Customers\" refers to the customers who defaulted on a loan. and \"Good Customers\" refers to the customers who paid back loan. WOE can be calculated using the below formula:\n",
    "\n",
    "$$WOE = ln \\left( \\frac{\\%   of  Non\\_Events}{\\%   of  Events} \\right)$$\n",
    "\n",
    "Steps to calculate WOE\n",
    "* For a continuous variable, split data into 10 parts (or lesser depending on the distribution).\n",
    "* Calculate the number of events and non-events in each group (bin)\n",
    "* Calculate the % of events and % of non-events in each group.\n",
    "* Calculate WOE by taking natural log of division of % of non-events and % of events\n",
    "\n",
    "**Information value** is one of the most useful technique to select important variables in a predictive model. It helps to rank variables on the basis of their importance. The IV is calculated using the following formula :\n",
    "$$IV = ∑ (\\% of Non\\_Events - \\% of Events) * WOE$$\n",
    "\n",
    "Read more about `WOE` and `IV` from the following [link](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_NB_MiniProject_2_Credit_risk_modelling_Logistic_Regression_WoE_and_IV.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-latter",
   "metadata": {
    "id": "operating-latter"
   },
   "source": [
    "## Grading = 10 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-syndrome",
   "metadata": {
    "id": "caring-syndrome"
   },
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-delay",
   "metadata": {
    "cellView": "form",
    "id": "comparable-delay"
   },
   "outputs": [],
   "source": [
    "# @title Download Dataset\n",
    "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/GiveMeSomeCredit.csv\n",
    "#!pip -qq install xverse\n",
    "print(\"Data Downloaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5nHbUGBPFdIC",
   "metadata": {
    "id": "5nHbUGBPFdIC"
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "jc2WLiGnFg-p",
   "metadata": {
    "id": "jc2WLiGnFg-p"
   },
   "outputs": [],
   "source": [
    "#!pip install pandas==1.3.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1mqj1J8TFjGS",
   "metadata": {
    "id": "1mqj1J8TFjGS"
   },
   "outputs": [],
   "source": [
    "#!pip install xverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-pattern",
   "metadata": {
    "id": "appreciated-pattern"
   },
   "source": [
    "### Import Neccesary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "loose-marsh",
   "metadata": {
    "id": "loose-marsh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from xverse.transformer import MonotonicBinning, WOE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-reflection",
   "metadata": {
    "id": "compressed-reflection"
   },
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-graph",
   "metadata": {
    "id": "fatty-graph"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df = pd.read_csv(\"GiveMeSomeCredit.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-sleeping",
   "metadata": {
    "id": "experienced-sleeping"
   },
   "source": [
    "#### Describe the all statistical properties of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-methodology",
   "metadata": {
    "id": "greek-methodology"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab25e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-hamilton",
   "metadata": {
    "id": "christian-hamilton"
   },
   "source": [
    "### Pre-processing (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-decision",
   "metadata": {
    "id": "global-decision"
   },
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "pharmaceutical-latvia",
   "metadata": {
    "id": "pharmaceutical-latvia"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-elimination",
   "metadata": {
    "id": "usual-elimination"
   },
   "source": [
    "#### Handle the missing data\n",
    "\n",
    "Find the how many null values in the dataset and fill with mean or remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-findings",
   "metadata": {
    "id": "heated-findings"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f7af01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MonthlyIncome\"] = df[\"MonthlyIncome\"].fillna(df[\"MonthlyIncome\"].median())\n",
    "df[\"NumberOfDependents\"] = df[\"NumberOfDependents\"].fillna(\n",
    "    df[\"NumberOfDependents\"].median()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-reply",
   "metadata": {
    "id": "hispanic-reply"
   },
   "source": [
    "### EDA &  Visualization ( 1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de31543",
   "metadata": {},
   "source": [
    "#### Assumption\n",
    "1. feature `SeriousDlqin2yrs` is a categorical variable and has two values with the meaning:\n",
    "    1. `0` : person is a not a defaulter\n",
    "    1. `1` : person is a defaulter\n",
    "1. All features except `SeriousDlqin2yrs` are numerical variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-cheese",
   "metadata": {
    "id": "standing-cheese"
   },
   "source": [
    "#### Calculate the percentage of the target labels and visualize with a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "958c6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-hands",
   "metadata": {
    "id": "attractive-hands"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "target_value_count = df[\"SeriousDlqin2yrs\"].value_counts()\n",
    "print(target_value_count)\n",
    "\n",
    "# target_value_count.plot(kind=\"bar\")\n",
    "non_defaulter, defaulter = target_value_count[0], target_value_count[1]\n",
    "y = np.array([non_defaulter, defaulter])\n",
    "y_percent = np.round(y / y.sum() * 100, 2)\n",
    "x = np.array([\"Non-Defaulter\", \"Defaulter\"])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(x, y)\n",
    "plt.text(\n",
    "    0,\n",
    "    non_defaulter,\n",
    "    non_defaulter,\n",
    "    # f\"{(non_defaulter / (non_defaulter + defaulter)) * 100:.3f}%\",\n",
    "    fontdict={\n",
    "        \"fontsize\": 12,\n",
    "        \"fontweight\": \"bold\",\n",
    "        \"color\": \"black\",\n",
    "        \"ha\": \"center\",\n",
    "        \"va\": \"bottom\",\n",
    "    },\n",
    ")\n",
    "plt.text(\n",
    "    1,\n",
    "    defaulter,\n",
    "    defaulter,\n",
    "    # f\"{(defaulter / (non_defaulter + defaulter)) * 100:.3f}%\",\n",
    "    fontdict={\n",
    "        \"fontsize\": 12,\n",
    "        \"fontweight\": \"bold\",\n",
    "        \"color\": \"black\",\n",
    "        \"ha\": \"center\",\n",
    "        \"va\": \"bottom\",\n",
    "    },\n",
    ")\n",
    "plt.xlabel(\"Credit Status\")\n",
    "plt.ylabel(\"Number of People\")\n",
    "plt.title(\"Credit Defaulter Status\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(y, labels=x, autopct=\"%1.1f%%\", explode=[0.2, 0], shadow=True, startangle=0)\n",
    "plt.legend(title=\"Credit Status\", loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-stopping",
   "metadata": {
    "id": "satisfactory-stopping"
   },
   "source": [
    "#### Plot the distribution of SeriousDlqin2yrs by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ages = df[\"age\"].nunique()\n",
    "print(f\"Unique ages: {unique_ages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    data=df,\n",
    "    x=\"age\",\n",
    "    hue=\"SeriousDlqin2yrs\",\n",
    "    multiple=\"stack\",\n",
    "    bins=unique_ages,\n",
    "    kde=True,\n",
    ")\n",
    "plt.title(\"Distribution of SeriousDlqin2yrs by Age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"SeriousDlqin2yrs\", labels=[\"Defaulter\", \"Non-Defaulter\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-rolling",
   "metadata": {
    "id": "promotional-rolling"
   },
   "source": [
    "#### Calculate the correlation and plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# keep a copy of the original dataframe\n",
    "df_ = df.copy(deep=True)\n",
    "df_.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635261ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segregate the continuous and categorical variables\n",
    "all_columns = df.columns.tolist()\n",
    "print(f\"All Columns: {all_columns}\")\n",
    "\n",
    "df_cat_var_columns = [\"SeriousDlqin2yrs\"]\n",
    "df_cont_var_columns = [val for val in all_columns if val not in df_cat_var_columns]\n",
    "\n",
    "print(f\"Categorical Variables: {df_cat_var_columns}\")\n",
    "print(f\"Continuous Variables: {df_cont_var_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the standard scaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "df[df_cont_var_columns] = ss.fit_transform(df[df_cont_var_columns])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-candidate",
   "metadata": {
    "id": "studied-candidate"
   },
   "outputs": [],
   "source": [
    "# plot the heat map\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.corr(), annot=True, linewidth=0.5, center=0, cmap=\"coolwarm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df42158",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "1. `age` has the lowest corelation with defaulting on credit.\n",
    "1. Historically, people who have cleared there installments are more likely to default again:\n",
    "    1. 30 to 59 days post due date\n",
    "    1. 60 to 89 days after due date \n",
    "    1. exactly 90 days after due date\n",
    "1. `NumberOfTime30-59DaysPastDueNotWorse`, `NumberOfTime60-89DaysPastDueNotWorse` and `NumberOfTimes90DaysLate` are highly correlated and hence only one of the field is sufficient to carry out the prediction.\n",
    "\n",
    "##### Questions\n",
    "1. Corelation between age and defaulter was counter intuitive for me. Can you confirm if the observation is correct?\n",
    "1. Can we draw some parallels between the SHAPley Heatmap and Corelation Heatmap?\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-minute",
   "metadata": {
    "id": "operational-minute"
   },
   "source": [
    "### Data Engineering (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-telephone",
   "metadata": {
    "id": "outer-telephone"
   },
   "source": [
    "#### Weight of Evidence and Information value\n",
    "\n",
    "* Arrange the binning for each variable with different bins\n",
    "    * For eg. Age = 49, Age_quantile_range = (48, 56)\n",
    "* Calculate information value and chooose the best features based on the rules given below\n",
    "\n",
    "| Information Value |\tVariable Predictiveness |\n",
    "| --- | --- |\n",
    "| Less than 0.02\t|  Not useful for prediction |\n",
    "| 0.02 to 0.1\t| Weak predictive Power |\n",
    "|  0.1 to 0.3 | Medium predictive Power |\n",
    "| 0.3 to 0.5 | Strong predictive Power |\n",
    "| >0.5 | Suspicious Predictive Power |\n",
    "\n",
    "* Calculate Weight of evidence for the selected variables\n",
    "\n",
    "Hint: Use [xverse](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_NB_MiniProject_2_Credit_risk_modelling_Logistic_Regression_Xverse.pdf). It is a machine learning Python module in the space of feature engineering, feature transformation and feature selection. It provides pre-built functions for the above steps, such as binning and conversion to WoE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-knock",
   "metadata": {
    "id": "ordered-knock"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# separate the target variable and the features\n",
    "y = df[\"SeriousDlqin2yrs\"]\n",
    "X = df.drop(\"SeriousDlqin2yrs\", axis=1)\n",
    "\n",
    "print(f\"features : {X.columns}\")\n",
    "print(f\"target : {y.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ce295",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MonotonicBinning()\n",
    "\n",
    "# It's a 6 years old code. Is it maintained? https://github.com/Sundar0989/XuniVerse/tree/master\n",
    "# My code doesn't compile with the current version of the package. I am commenting this code.\n",
    "\n",
    "# clf.fit(X, y)\n",
    "# print(clf.bins)  # print the bins\n",
    "\n",
    "# output_bins = clf.bins  # store the bins in a variable\n",
    "# clf = MonotonicBinning(\n",
    "#     custom_binning=output_bins\n",
    "# )  # initialize the MonotonicBinning class\n",
    "\n",
    "# out_X = clf.transform(X)  # transform the features\n",
    "# out_X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = WOE()\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# clf.woe_df  # print the WOE values\n",
    "# clf.iv_df  # print the IV values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-rebel",
   "metadata": {
    "id": "conservative-rebel"
   },
   "source": [
    "### Identify features,  target and split it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ambient-dress",
   "metadata": {
    "id": "ambient-dress"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=101, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-sucking",
   "metadata": {
    "id": "decreased-sucking"
   },
   "source": [
    "### Logistic Regression from scratch using gradient method (2 points)\n",
    "\n",
    "For Linear Regression, we had the hypothesis $yhat = w.X +b$ , whose output range was the set of all Real Numbers.\n",
    "Now, for Logistic Regression our hypothesis is  $yhat = sigmoid(w.X + b)$ , whose output range is between 0 and 1 because by applying a sigmoid function, we always output a number between 0 and 1.\n",
    "\n",
    "$yhat = \\frac{1}{1 +e^{-(w.x+b)}}$\n",
    "\n",
    "Hint: [logistic-regression-with-python](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/odsc%20%20Logistic.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-business",
   "metadata": {
    "id": "precious-business"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def sigmoid(x):\n",
    "    return np.maximum(np.minimum(1 / (1 + np.exp(-x)), 0.9999), 0.0001)\n",
    "\n",
    "\n",
    "def cost_function(x, y, theta):\n",
    "    t = x.dot(theta)\n",
    "    # return —1 * np.sum(y * np.log(sigmoid(t)) + (1 — y) * np.log(1 — sigmoid(t))) / x.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-black",
   "metadata": {
    "id": "reliable-black"
   },
   "source": [
    "### Implement the Logistic regression using sklearn (2 points)\n",
    "\n",
    "As there is imbalance in the class distribution, add weightage to the Logistic regression.\n",
    "\n",
    "* Find the accuracy with class weightage in Logistic regression\n",
    "* Find the accuracy without class weightage in Logistic regression\n",
    "\n",
    "Hint: [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics using pandas Series operations\n",
    "    \"\"\"\n",
    "    # Convert inputs to pandas Series if they aren't already\n",
    "    y_true = pd.Series(y_true)\n",
    "    y_pred = pd.Series(y_pred)\n",
    "\n",
    "    # Calculate metrics using pandas operations\n",
    "    TP = sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    # Calculate derived metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = (\n",
    "        2 * (precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"TP\": TP,\n",
    "            \"TN\": TN,\n",
    "            \"FP\": FP,\n",
    "            \"FN\": FN,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-assistant",
   "metadata": {
    "id": "impressive-assistant"
   },
   "outputs": [],
   "source": [
    "# With weightage\n",
    "# YOUR CODE HERE\n",
    "classifier_no_weights = LogisticRegression(random_state=101)\n",
    "classifier_no_weights.fit(X_train, y_train)\n",
    "y_pred_no_weights = classifier_no_weights.predict(X_test)\n",
    "\n",
    "# calculate the accuracy too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-flower",
   "metadata": {
    "id": "similar-flower"
   },
   "outputs": [],
   "source": [
    "# Without weightage\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = dict(\n",
    "    df['SeriousDlqin2yrs'].value_counts(normalize=True).apply(lambda x: df.shape[0]/(2*df.shape[0]*x))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78158a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_with_weights = LogisticRegression(random_state=101, class_weight=class_weights)\n",
    "classifier_with_weights.fit(X_train, y_train)\n",
    "y_pred_with_weights = classifier_with_weights.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd817d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced dataset using pandas\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_class_0 = int(0.93 * n_samples)\n",
    "n_class_1 = n_samples - n_class_0\n",
    "\n",
    "# Create DataFrame with features and target\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature1\": np.concatenate(\n",
    "            [np.random.normal(0, 1, n_class_0), np.random.normal(2, 1, n_class_1)]\n",
    "        ),\n",
    "        \"feature2\": np.concatenate(\n",
    "            [np.random.normal(0, 1, n_class_0), np.random.normal(2, 1, n_class_1)]\n",
    "        ),\n",
    "        \"target\": np.concatenate([np.zeros(n_class_0), np.ones(n_class_1)]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Split features and target\n",
    "X = df[[\"feature1\", \"feature2\"]]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train model without weights\n",
    "model_no_weights = LogisticRegression(random_state=42)\n",
    "model_no_weights.fit(X_train, y_train)\n",
    "y_pred_no_weights = model_no_weights.predict(X_test)\n",
    "\n",
    "# Calculate class weights using pandas\n",
    "class_weights = dict(\n",
    "    df[\"target\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .apply(lambda x: n_samples / (2 * n_samples * x))\n",
    ")\n",
    "\n",
    "# Train model with weights\n",
    "model_with_weights = LogisticRegression(random_state=42, class_weight=class_weights)\n",
    "model_with_weights.fit(X_train, y_train)\n",
    "y_pred_with_weights = model_with_weights.predict(X_test)\n",
    "\n",
    "# Calculate metrics for both models\n",
    "metrics_no_weights = calculate_metrics(y_test, y_pred_no_weights)\n",
    "metrics_with_weights = calculate_metrics(y_test, y_pred_with_weights)\n",
    "\n",
    "# Create DataFrame with all metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\"Without Weights\": metrics_no_weights, \"With Weights\": metrics_with_weights}\n",
    ")\n",
    "\n",
    "# Create confusion matrices using pandas crosstab\n",
    "conf_matrix_no_weights = pd.crosstab(\n",
    "    y_test, pd.Series(y_pred_no_weights), rownames=[\"Actual\"], colnames=[\"Predicted\"]\n",
    ")\n",
    "\n",
    "conf_matrix_with_weights = pd.crosstab(\n",
    "    y_test, pd.Series(y_pred_with_weights), rownames=[\"Actual\"], colnames=[\"Predicted\"]\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Class Distribution in Training Data:\")\n",
    "print(df[\"target\"].value_counts(normalize=True).mul(100).round(2), \"\\n\")\n",
    "\n",
    "print(\"Class Weights Used:\")\n",
    "print(pd.Series(class_weights), \"\\n\")\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(metrics_df.round(3), \"\\n\")\n",
    "\n",
    "print(\"\\nConfusion Matrix - Without Weights:\")\n",
    "print(conf_matrix_no_weights)\n",
    "\n",
    "print(\"\\nConfusion Matrix - With Weights:\")\n",
    "print(conf_matrix_with_weights)\n",
    "\n",
    "# Optional: Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot feature distribution\n",
    "sns.scatterplot(data=df, x=\"feature1\", y=\"feature2\", hue=\"target\", ax=ax1, alpha=0.6)\n",
    "ax1.set_title(\"Feature Distribution\")\n",
    "\n",
    "# Plot metrics comparison\n",
    "metrics_df.loc[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot(kind=\"bar\", ax=ax2)\n",
    "ax2.set_title(\"Model Metrics Comparison\")\n",
    "ax2.set_ylabel(\"Score\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-error",
   "metadata": {
    "id": "signal-error"
   },
   "source": [
    "### Credit scoring (1 point)\n",
    "\n",
    "When scaling the model into a scorecard, we will need both the Logistic Regression coefficients from model fitting as well as the transformed WoE values. We will also need to convert the score from the model from the log-odds unit to a points system.\n",
    "For each independent variable Xi, its corresponding score is:\n",
    "\n",
    "$Score = \\sum_{i=1}^{n} (-(β_i × WoE_i + \\frac{α}{n}) × Factor + \\frac{Offset}{n})$\n",
    "\n",
    "Where:\n",
    "\n",
    "βi — logistic regression coefficient for the variable Xi\n",
    "\n",
    "α — logistic regression intercept\n",
    "\n",
    "WoE — Weight of Evidence value for variable Xi\n",
    "\n",
    "n — number of independent variable Xi in the model\n",
    "\n",
    "Factor, Offset — known as scaling parameter\n",
    "\n",
    "  - Factor = pdo / ln(2); pdo is points to double the odds\n",
    "  - Offset = Round_of_Score - {Factor * ln(Odds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-spare",
   "metadata": {
    "id": "worst-spare"
   },
   "outputs": [],
   "source": [
    "# Scaling factors\n",
    "factor = 20 / np.log(2)\n",
    "offset = 600 - (factor * np.log(50))\n",
    "factor, offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XwwnwQKMU_Nx",
   "metadata": {
    "id": "XwwnwQKMU_Nx"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-internship",
   "metadata": {
    "id": "intelligent-internship"
   },
   "source": [
    "### Performance Metrics (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-hygiene",
   "metadata": {
    "id": "innocent-hygiene"
   },
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-listening",
   "metadata": {
    "id": "optimum-listening"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-keyboard",
   "metadata": {
    "id": "accessory-keyboard"
   },
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-corner",
   "metadata": {
    "id": "civic-corner"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-amendment",
   "metadata": {
    "id": "wired-amendment"
   },
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-machinery",
   "metadata": {
    "id": "impossible-machinery"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-feelings",
   "metadata": {
    "id": "dense-feelings"
   },
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-remains",
   "metadata": {
    "id": "running-remains"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RR-ivkGyqJXs",
   "metadata": {
    "id": "RR-ivkGyqJXs"
   },
   "source": [
    "### Report Analysis\n",
    "\n",
    "* Comment on the performance of the model with weightage and without weightage\n",
    "* Have you tried implementing Logistic regression with normal features instead of WOE ?\n",
    "  - Compare the classification report for both implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kas4-RzF6u7y",
   "metadata": {
    "id": "kas4-RzF6u7y"
   },
   "source": [
    "## SHAP implementation for Logistic Regression (Optional)\n",
    "\n",
    "Implement SHAP and understand the underlying reasons or factors that are maximally influencing the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qhBunQ-77JCw",
   "metadata": {
    "id": "qhBunQ-77JCw"
   },
   "source": [
    "**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions.\n",
    "\n",
    "In the below figure, we can see how the different feature values (Age, Sex, BMI, etc) are affecting the base value (0.1) to give the final output prediction (0.4). The base value or the expected value is the average of the model output over the training data X_train.\n",
    "\n",
    "![](https://cdn.iisc.talentsprint.com/CDS/Images/Shap_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TozINzm67Y4o",
   "metadata": {
    "id": "TozINzm67Y4o"
   },
   "source": [
    "To understand how to compute and interpet Shapley-based explanations of a machine learning model, we will use the following plots:\n",
    "\n",
    "- Force plot\n",
    "- Feature importance plot\n",
    "- Summary plot\n",
    "- Dependence plot\n",
    "- Clustering Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vHg8XxZj7dUY",
   "metadata": {
    "id": "vHg8XxZj7dUY"
   },
   "outputs": [],
   "source": [
    "!pip -qq install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rX8AH1nW7gRB",
   "metadata": {
    "id": "rX8AH1nW7gRB"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lkciaNYj7k24",
   "metadata": {
    "id": "lkciaNYj7k24"
   },
   "source": [
    "### Force plot for individual instances (Optional)\n",
    "\n",
    "We can visualize feature attributions such as Shapley values as “forces”. Each feature value is a force that either increases or decreases the prediction. The prediction starts from the baseline. The baseline for Shapley values is the average of all predictions.\n",
    "\n",
    "In the plot, each Shapley value should be represented as an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance.\n",
    "\n",
    "**Hint:** Use `shap.force_plot()` function that takes three values:\n",
    "\n",
    "- the base value (explainer.expected_value),\n",
    "- the SHAP values, and\n",
    "- the matrix of feature values\n",
    "\n",
    "Show the SHAP force plots for two instances from the GiveMeSomeCredit dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zNURb_wP72sK",
   "metadata": {
    "id": "zNURb_wP72sK"
   },
   "outputs": [],
   "source": [
    "# Instead of using the whole training set to estimate expected values, we summarize with\n",
    "# a set of weighted kmeans, each weighted by the number of points they represent.\n",
    "# summarize the background as K samples. Use 'shap.kmeans()' and\n",
    "# store it in a variable 'xtrain_summary'\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hXDMp0ft-HOP",
   "metadata": {
    "id": "hXDMp0ft-HOP"
   },
   "outputs": [],
   "source": [
    "# Convert the selected columns into list\n",
    "feature_names = selected_columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Z9KR11986lH",
   "metadata": {
    "id": "0Z9KR11986lH"
   },
   "outputs": [],
   "source": [
    "# Force plot for an instance\n",
    "shap.initjs()\n",
    "explainer = shap.KernelExplainer(log_reg.predict, xtrain_summary)\n",
    "shap_value = explainer.shap_values(xtest.iloc[0, :].values)\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_value,\n",
    "    xtest.iloc[0, :].values,\n",
    "    feature_names=feature_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kZoj8BPG-StQ",
   "metadata": {
    "id": "kZoj8BPG-StQ"
   },
   "source": [
    "In the above plot:\n",
    "\n",
    "- The output value **f(x)** is the prediction for that observation (the predicted output value of the first row in X_test is ≈ **0**, indicating low credit risk).\n",
    "- The **base value**: is “the value that would be predicted if we did not know any features for the current output.” Here it is **0.004808**.\n",
    "- Red/blue: Features that push the prediction value higher (to the right) are shown in red, and those pushing the prediction value lower are in blue.\n",
    "\n",
    "The features are having risk decreasing effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qKnjCee286mJ",
   "metadata": {
    "id": "qKnjCee286mJ"
   },
   "outputs": [],
   "source": [
    "# Force plot for another instance\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FsE2CIuo_AvK",
   "metadata": {
    "id": "FsE2CIuo_AvK"
   },
   "source": [
    "From the above plot, put your remarks on the output value **f(x)** i.e, the predicted output value for that observation is ≈ **0**.\n",
    "\n",
    "Put your remarks on predicted risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RmHZq5NG_iqu",
   "metadata": {
    "id": "RmHZq5NG_iqu"
   },
   "source": [
    "Shapley values can be combined into global explanations. If we run SHAP for every instance, we get a matrix of Shapley values. This matrix has one row per data instance and one column per feature. We can interpret the entire model by analyzing the Shapley values in this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G3sT-msd_nY-",
   "metadata": {
    "id": "G3sT-msd_nY-"
   },
   "source": [
    "### SHAP Feature Importance (Optional)\n",
    "\n",
    "The idea behind SHAP feature importance is simple: Features with large **absolute** Shapley values are important. Since we want the global importance, we average the absolute Shapley values per feature across the data. Next, we sort the features by decreasing importance and plot them.\n",
    "\n",
    "Plot the SHAP feature importance for the logistic regression model trained before for predicting credit risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juMkosFJ_zwg",
   "metadata": {
    "id": "juMkosFJ_zwg"
   },
   "source": [
    "**Hint:** Use the `shap.summary_plot` function with `plot_type=”bar”` to produce the feature importance plot. It lists the most significant features in descending order. The top variables contribute more to the model than the bottom ones and thus have high predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AJ4QS2QoACI-",
   "metadata": {
    "id": "AJ4QS2QoACI-"
   },
   "outputs": [],
   "source": [
    "# Get SHAP values\n",
    "shap_values = explainer.shap_values(xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LqMKYSswAEgY",
   "metadata": {
    "id": "LqMKYSswAEgY"
   },
   "outputs": [],
   "source": [
    "# Plot the Feature importance. Use 'shap.summary_plot()'\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5_fsxguATJ2",
   "metadata": {
    "id": "t5_fsxguATJ2"
   },
   "source": [
    "For a more informative plot, we will look at the summary plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KkCy98USAXNG",
   "metadata": {
    "id": "KkCy98USAXNG"
   },
   "source": [
    "### SHAP Summary Plot (Optional)\n",
    "\n",
    "The summary plot combines feature importance with feature effects:\n",
    "\n",
    "* Each point on the summary plot shoukd indicate a Shapley value for a feature and an instance.\n",
    "* The position on the y-axis should be determined by the feature and on the x-axis by the Shapley value.\n",
    "* Overlapping points should be appearing jittered in y-axis direction, to get a sense of the distribution of the Shapley values per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZZAXRxa1Arb2",
   "metadata": {
    "id": "ZZAXRxa1Arb2"
   },
   "outputs": [],
   "source": [
    "# Show the Summary plot. Use 'shap.summary_plot()'\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xZPUrha2A3ds",
   "metadata": {
    "id": "xZPUrha2A3ds"
   },
   "source": [
    "In the summary plot, we see first indications of the relationship between the value of a feature and the impact on the prediction. But to see the exact form of the relationship, we have to look at SHAP dependence plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DC-uu3wZBEfu",
   "metadata": {
    "id": "DC-uu3wZBEfu"
   },
   "source": [
    "### SHAP Dependence Plot (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HOBz0-bpBKkf",
   "metadata": {
    "id": "HOBz0-bpBKkf"
   },
   "source": [
    "The partial dependence plot shows the marginal effect of one or two features have on the predicted outcome of a machine learning model. It tells whether the relationship between the target and a feature is linear, monotonic or more complex.\n",
    "\n",
    "To implement SHAP feature dependence plot:\n",
    "\n",
    "* Pick a feature\n",
    "* For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis\n",
    "\n",
    "**Hint:** In order to create a dependence plot, we use `shap.dependence_plot()` function. The function automatically includes another variable that the chosen variable interacts most with. The following plot should show the relationship between “age” and the target variable, and “age” interacts with “NumberOfTime60-89DaysPastDueNotWorse” feature frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i0-BxNVgBkNm",
   "metadata": {
    "id": "i0-BxNVgBkNm"
   },
   "outputs": [],
   "source": [
    "# Show the Dependence plot. Use 'shap.dependence_plot()'\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F78xpqVPCAfW",
   "metadata": {
    "id": "F78xpqVPCAfW"
   },
   "source": [
    "### Clustering Shapley Values (Optional)\n",
    "\n",
    "We can cluster the data with the help of Shapley values. The goal of clustering is to find groups of similar instances.\n",
    "\n",
    "SHAP clustering works by clustering the Shapley values of each instance. This means that we cluster instances by explanation similarity. All SHAP values have the same unit – the unit of the prediction space. We can use any clustering method. In the following code cell, you need to use hierarchical agglomerative clustering to order the instances.\n",
    "\n",
    "The plot should consist of many force plots, each of which will explain the prediction of an instance. Rotate the force plots vertically and place them side by side according to their clustering similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IM0ka8zuCq_n",
   "metadata": {
    "id": "IM0ka8zuCq_n"
   },
   "outputs": [],
   "source": [
    "# Show the Force plot for first 100 instances of xtest\n",
    "shap.initjs()\n",
    "\n",
    "# Use shap.force_plot()\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1l7QHnbDPbH",
   "metadata": {
    "id": "i1l7QHnbDPbH"
   },
   "source": [
    "Put your remarks here regarding the above plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FcHezardDbQ8",
   "metadata": {
    "id": "FcHezardDbQ8"
   },
   "source": [
    "Also, we can show the force_plot() for the entire set using the below code cell. Note that it may take more than 30 minutes to run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2UkQT67Dqxm",
   "metadata": {
    "id": "p2UkQT67Dqxm"
   },
   "outputs": [],
   "source": [
    "# Force plot for entire xtest\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer.expected_value, shap_values, xtest.values, feature_names = feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
